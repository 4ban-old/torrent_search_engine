Размышления о релевантности. Часть 1 - Изучаем текст.
=================================================

.. epigraph::

   *"Математику уже затем учить следует, что она ум в порядок приводит."*
   
   -- М.В.Ломоносов.

Начало
------

Любой текст можно представить как последовательность слов. Не просто набор, а последовательность, так как позиция слова может иметь значение. Для выяснения значимости слов в тексте, необходимо определиться с некоторыми понятиями и представлениями, которые я буду развивать в ходе рассуждений.

Итак, любой текст несет информацию. Не имеет значения на сколько велик текст (сколько в нем разных и одинаковых слов), любой отдельно взятый текст несет 100% информации, таким образом, учитывая, что текст это набор слов, можно говорить о том, что **слово** - это мельчайшая неделимая информационная частица текста.

Напрашивается мысль, что слова, как носители информации, равноценны между собой. Это необязательно так, но с этого мы начнем. Такой текст, где каждый элемент равноценен с остальными, я буду называть **Равномерным текстом**.

Равномерный текст
-----------------
     
Пусть *t* - некоторая последовательность слов (текст): :math:`(a_1, a_2, \ldots, a_n)`.

Назовем :math:`T_t` множество уникальных слов текста t: :math:`(a_1, a_2, \ldots, a_k)`.

Назовем длиной текста *t* количество всех слов в тексте: :math:`L_t=n`.

Любое слово в тексте это минимальный носитель информации, причем значимость разных слов, как носителей информации, одинакова, следовательно количесво всех слов или, что тоже, длина текста :math:`L_t`, и составляют 100% информации текста. Иначе говоря, - информация равномерно распределена между словами по всей длине текста.

Назовем :math:`I_t` - величину, показывающую информационную значимость текста. Так как отдельно взятый текст несет в себе 100% информации, то его информационная значимость и есть эти 100%. Какую бы величину :math:`I_t` мы не взяли для определения значимости равномерного текста в информационном плане, эта величина равномерно распределиться по всей длине текста.

Таким образом, каково бы ни было :math:`I_t`, информационная значимость любого слова в определенной позиции в тексте

.. math::
   I(i,t)=\frac{I_t}{L_t}.

С учетом того, что слова могут повторяться, полную информационную значимость слова *a* в тексте *t* можно определить, как сумму всех информационных значимостей слова *а* для каждой позиции:

.. math::
   I(a,t) = \underbrace{\frac{I_t}{L_t}+\frac{I_t}{L_t}+\cdots+\frac{I_t}{L_t}}_{n_a-times},

где :math:`n_a=N(a,t)` - число вхождений слова *а* в текст *t*.

Или, что тоже:

.. math::
   I(a, t)=\frac{N(a, t)}{L_t}I_t

где :math:`N(a,t)` - количество вхождений a в t, :math:`L_t` - длина (количество всех слов) текста t, :math:`I_t` - информационная значимость текста

Очевидно, что значение информационной значимости слова :math:`I(a,t)` недостаточно для показателя веса слова :math:`W(a,t)`, так как оно опирается на информационную значимость текста :math:`I_t`. Вес слова должен показывать долевой вклад в единицу информации, которую несет весь текст, поэтому для получения веса слова, мы разделим информационную значимость слова на информационную значимость текста:

.. math::
   W(a, t) &= \frac{I(a, t)}{I_t} = \frac{\frac{N(a,t)}{L_t}I_t}{I_t} = \\
   &= \frac{N(a, t)}{L_t}

**Вес** слова *a* в равномерном тексте *t* это отношение количества вхождений слова *а* в текст *t* к количеству всех слов составляющих текст *t*.

Для всех уникальных слов :math:`T_t=(a_1, a_2, \cdots, a_k)` можно составить множество весов этих слов :math:`W_t=(W(a_1, t), W(a_2, t), \cdots, W(a_k, t))`. Очевидно, что сумма всех весов слов составит единицу.

.. math::
   \sum^n_{i=0}W_t(i) &= 1

Неравномерный текст
-------------------

Допустим, что информация между словами в тексте распределена неравномерно. Необходимо предполагать, что каждое слово *a* в неравномерном тексте :math:`t^{'}` длины *n* емеет собственный коэффициент значимости для той позиции, где находится. Тогда, очевидно, значимость всего текста не что иное, как сумма значимости всех слов в позициях:

.. math::
   I_{t^{'}}=\sum_{i=1}^{n}I(a_i,t^{'})
    
где :math:`I(a_i, t^{'})` информационная значимость слова в позиции *i*

а вес слова в тексте будет отношением суммы всех информационных значений слова (для каждой позиции, где он встречается) к общей значимости.

.. math::
   W(a, t^{'}) = \frac{\sum_{C(a, i)}I(a_i, t^{'})}{\sum_{i=1}^{n}I(i,t^{'})} = \frac{\sum_{C(a, i)}I(a_i, t^{'})}{I_{t^{'}}}

Также, как и в равномерном тексте, для всех уникальных слов неравномерного текста :math:`T_{t^{'}}=(a_1, a_2, \cdots, a_k)` можно составить множество весов этих слов :math:`W_{t^{'}}=(W(a_1, t^{'}), W(a_2, t^{'}), \cdots, W(a_k, t^{'}))`. Очевидно, что сумма всех весов слов составит единицу.

**Например**, необходимо учесть порядок слов в тексте таким образом, что, чем раньше остальных находится слово, тем больший вес оно имеет.

Текст неравномерен, причем информационная значимость слов равномерно убывает от начала до конца всего текста. Можно представить последовательность информационных значимостей слов, как последовательность натуральных чисел начиная с длины текста и заканчивая единицей c шагом -1. Итак, пусть *t* - описаный неравномерный текст :math:`(a_1, a_2, a_3, \ldots, a_n)`, тогда информационные значимости слов :math:`(n, n-1, n-2, \ldots, 1)`:

.. math::
   a_1 &\to n \\
   a_2 &\to n-1 \\
   a_3 &\to n-2 \\
   &\ldots \\
   a_i &\to n-(i-1)\\

то есть информационная значимость слова *a* в позиции *i* текста *t*:

.. math::
   I(a_i, t) = L_t - (i-1)

Полная информационная значимость текста :math:`I_t` равна сумме всех информационных значимостей слов, то есть сумме членов натурального ряда от 1 до длины текста :math:`L_t`.

.. math::
   I_t = \sum_{n=1}^{L_t}n = \frac{L_t(L_t+1)}{2}

Отсюда вес слова *a* в тексте *t* можно вычислить, как:

.. math::
   W(a, t) = \frac{\sum_{C(a,i)}I(a_i, t)}{\frac{L_t(L_t+1)}{2}} = \frac{2\sum_{C(a,i)}I(a_i,t)}{L_t(L_t+1)}

Уникальные слова и их вес
-------------------------

Внимательно оглядывая вышесказанное, можно обобщить понятия **Равномерного текста** и **Неравномерного текста**. Например, хорошо видно, что равномерное распределение информации между словами в тексте это частный случай неравномерного текста, где информационная значимость слова :math:`I(a_i, t) = 1, \forall i`.

Если распределение информации между словами поддается исчислению, мы всегда можем составить множество уникальных слов :math:`T_t` и соответствующее множество весов слов :math:`W_t`. Эти два множества вкупе, полностью характеризуют текст с информационной стороны и это важный момент.

Обобщая дальше, можно сказать, что множество :math:`T_t` это набор всех **элементов** текста, а множество :math:`W_t` соответствущие этим элементам веса. Почему я теперь говорю 'элементов', вместо 'слов' и что это дает? Предполагаю, что текст это нечто более сложное, нежели последовательность слов. Осмысленный текст всегда состоит из множества частей, такие как эпиграф, заголовоки, абзацы, сноски, выделения внутри текста (например жирным шрифтом). По-сути это разнообразие не что иное, как распределение информационной значимости для различных элементов текста, которое мы воспринимаем интуитивно. Необходимо разобраться что это за элементы и как с ними обращаться.

Блоки текста
------------

Итак, возьмем текст, состоящий из блоков. Пусть текст *t* состоит из *q* блоков :math:`t = t_1 + t_2 + \cdots + t_q`. Возьмем для изучения два соседних блока :math:`f = t_i` и :math:`g = t_{i+1}`.

:math:`f = (a_1, a_2, \ldots, a_{n_f}), g = (b_1, b_2, \ldots, b_{n_g})`

Каждый блок может по-своему распределять веса для слов, иметь свои собственные правила или наборы значимости слов, но, в конечном итоге каждый из них можно представить как набор уникальных слов и их весов. 
   
.. math::
   f \left\{ 
   \begin{aligned}
   T_f &= (a_1, a_2, \ldots, a_{k_f})\\
   W_f &= (W_{a_1}, W_{a_2}, \ldots, W_{a_{k_f}})
   \end{aligned}
   \right.
   \qquad g \left\{ 
   \begin{aligned}
   T_g &= (b_1, b_2, \ldots, b_{k_g})\\
   W_g &= (W_{b_1}, W_{b_2}, \ldots, W_{b_{k_g}})
   \end{aligned}
   \right.
   
Давайте представим полученные наборы уникальных элементов, как текст. :math:`f^{'}=(a_1, a_2, \ldots, a_{k_f}), \quad g^{'}=(b_1, b_2, \ldots, b_{f_g})` и обединим их в один текст :math:`\check{t}=(a_1, a_2, \ldots, a_{n_f}, b_1, b_2, \ldots, b_{n_g})`.
Для удобства обозначим его как :math:`\check{t}=(a_1, a_2, \ldots, a_n)`, где

.. math::
   n = n_f + n_g, \quad 
   a_i = \left\{
   \begin{aligned}
   a_i \in T_f,\quad i \le n_f \\
   b_{i-n_f} \in T_g,\quad i>n_f
   \end{aligned}
   \right.
   
Мы получили неравномерно распределенный текст :math:`\check{t}`. Ясно, что значения информационной значимости слов в позиции мы можем получить из весов слов. Но, в отличии от самих слов, мы не можем просто объединить множества весов. Вес слова это величина, показывающая долевое участие слова для всего блока текста, но текст, получившийся объединением множеств уникальных слов имеет другую величину. Необходимо пересчитать вес для каждого слова, с учетом длин исходных текстов :math:`L_{f^{'}}` и :math:`L_{g^{'}}` и длины текста получившегося объединением исходных :math:`L_{\check{t}}`
А именно, мы должны выяснить долевое участие исходного блока текста в объединенном.

.. math::
   W(f^{'}, \check{t}) = \frac{ L_{f^{'}} }{ L_{\check{t}} } = \frac{ L_{f^{'}} }{ L_{f^{'} }+L_{g^{'}} } \\
   W(g^{'}, \check{t}) = \frac{ L_{g^{'}} }{ L_{\check{t}} } = \frac{L_{g^{'}}}{L_{f^{'}}+L_{g^{'}}}
   
Теперь

.. math::
   I_{f^{'}} &=(W(1,f) \cdot W(f^{'}, \check{t}), W(2,f) \cdot W(f^{'}, \check{t}), \ldots, W(k_f,f) \cdot W(f^{'}, \check{t})) \\
   I_{g^{'}} &=(W(1,g) \cdot W(g^{'}, \check{t}), W(2,g) \cdot W(g^{'}, \check{t}), \ldots, W(k_f,g ) \cdot W(g^{'}, \check{t}))
   
:math:`I_{\check{t}}` теперь мы можем получить объединив эти наборы способом, аналогичным объединению слов.

Итак, у нас получился неравномерный текст :math:`\check{t}` со своим набором информационной значимости слов в позиции. Стоит заметить, что эти значения уже приведены к информационной значимости текста. Для неповторяющихся слов, они будут совпадать с их весом, для повторяющихся слов, достаточно суммировать значения повторений.

Нетрудно обобщить наши рассуждения, проведенные для двух соседних блоков, на все блоки, составляющие текст. Вернемся к нашему тексту :math:`t=t_1+t_2+\cdots+t_q`, Каждый из блоков текста, его составляющего можно представить набором уникальных слов и их весов, тогда наш текст будет составлен из уникальных наборов текстов:

.. math::
   t &= (\underbrace{T_{t_1}(1), T_{t_1}(2), \ldots, T_{t_1}(L_{T_{t_1}})}_{T_{t_1}}, \underbrace{T_{t_2}(1), T_{t_2}(2), \ldots, T_{t_2}(L_{T_{t_2}})}_{T_{t_2}}, \ldots, \underbrace{T_{t_q}(1), T_{t_q}(2), \ldots, T_{t_q}(L_{T_{t_q}})}_{T_{t_q}})
   
Длина получившегося текста :math:`L_t = L_{T_{t_1}}+L_{T_{t_2}}+\cdots + L_{T_{t_q}}`, можно вычислить долевое участие (**вес**) каждого блока и получить информационную значимость каждого слова в позиции:

.. math::
   \mu_1 &= \frac{L_{T_{t_1}}}{L_t} \quad
   \mu_2 = \frac{L_{T_{t_2}}}{L_t} \quad
   \cdots \quad
   \mu_q = \frac{L_{T_{t_q}}}{L_t}, \\
   
   I_t &= (\underbrace{\mu_1 W(1, t_1), \mu_1 W(2, t_1), \ldots, \mu_1 W(L_{W_{t_1}}, t_1)}_{W_{t_1}}, \underbrace{\mu_2 W(1, t_2), \mu_2 W(2, t_2), \ldots, \mu_2 W(L_{W_{t_2}}, t_2)}_{W_{t_2}}, \ldots \\ & \ldots, \underbrace{\mu_q W(1, t_q), \mu_q W(2, t_q), \ldots, \mu_q W(L_{W_{t_q}}, t_q)}_{W_{t_q}})
   
Мы получили неравномерный текст, для каждого слова которого определена информационная значимость в позиции. Как составить набор уникальных слов и рассчитать для них вес мы уже знаем, это дело техники.

Получившаяся структура, представляющая текст, состоит из блоков, у каждого из этих блоков есть своя величина долевого участия, свой вес, что очень напоминает неравномерный текст состоящий из слов. Фактически так оно и есть. Но вот только мы вичисляли  вес для каждого блока :math:`\mu` так, будто каждый блок был равноценен перед другими и его долевое участие зависит только от его длины. Тоесть мы рассматривали **Равномерные блоки**? А если они неравномерны?

Неравномерные блоки текста
--------------------------


   
   